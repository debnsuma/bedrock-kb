{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy improvement for Knowledge Bases for Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "Before running the rest of this notebook, you'll need to run the cells below to (ensure necessary libraries are installed and) connect to Bedrock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U opensearch-py==2.3.1 --quiet\n",
    "# %pip install -U boto3 --quiet\n",
    "# %pip install -U retrying==1.3.4 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a vector store - OpenSearch Serverless index\n",
    "\n",
    "### Step 1 - Create OSS policies and collection\n",
    "Firt of all we have to create a vector store. In this section we will use *Amazon OpenSerach serverless.*\n",
    "\n",
    "Amazon OpenSearch Serverless is a serverless option in Amazon OpenSearch Service. As a developer, you can use OpenSearch Serverless to run petabyte-scale workloads without configuring, managing, and scaling OpenSearch clusters. You get the same interactive millisecond response times as OpenSearch Service with the simplicity of a serverless environment. Pay only for what you use by automatically scaling resources to provide the right amount of capacity for your application—without impacting data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "from utility import create_bedrock_execution_role, create_oss_policy_attach_bedrock_execution_role, create_policies_in_oss\n",
    "import random\n",
    "from retrying import retry\n",
    "import time\n",
    "from pprint import pprint as pp \n",
    "\n",
    "suffix = random.randrange(200, 900)\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "boto3_session = boto3.session.Session()\n",
    "region_name = boto3_session.region_name\n",
    "\n",
    "bedrock_agent_client = boto3_session.client('bedrock-agent', region_name=region_name)\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\", region_name=region_name)\n",
    "\n",
    "service = 'aoss'\n",
    "s3_client = boto3.client('s3')\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "s3_suffix = f\"{region_name}-{account_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34.143\n"
     ]
    }
   ],
   "source": [
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'my-kb-dataset-test-bucket-2024'                                                          #### Provide your bucket name which is already created\n",
    "\n",
    "vector_store_name = f'bedrock-vectordb-rag-{suffix}'\n",
    "index_name = f\"bedrock-vectordb-rag-index-{suffix}\"\n",
    "aoss_client = boto3_session.client('opensearchserverless')\n",
    "bedrock_kb_execution_role = create_bedrock_execution_role(bucket_name)\n",
    "bedrock_kb_execution_role_arn = bedrock_kb_execution_role['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# create security, network and data access policies within OSS\n",
    "encryption_policy, network_policy, access_policy = create_policies_in_oss( vector_store_name=vector_store_name,\n",
    "                                                                           aoss_client=aoss_client,\n",
    "                                                                           bedrock_kb_execution_role_arn=bedrock_kb_execution_role_arn)\n",
    "collection = aoss_client.create_collection(name=vector_store_name,type='VECTORSEARCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'HTTPHeaders': {'connection': 'keep-alive',\n",
      "                                      'content-length': '316',\n",
      "                                      'content-type': 'application/x-amz-json-1.0',\n",
      "                                      'date': 'Thu, 18 Jul 2024 19:48:58 GMT',\n",
      "                                      'x-amzn-requestid': 'dd54881b-4048-4a3c-adbe-4bb73580c094'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'dd54881b-4048-4a3c-adbe-4bb73580c094',\n",
      "                      'RetryAttempts': 0},\n",
      " 'createCollectionDetail': {'arn': 'arn:aws:aoss:us-east-1:507922848584:collection/94qa58u59f50h2eiuwp3',\n",
      "                            'createdDate': 1721332138246,\n",
      "                            'id': '94qa58u59f50h2eiuwp3',\n",
      "                            'kmsKeyArn': 'auto',\n",
      "                            'lastModifiedDate': 1721332138246,\n",
      "                            'name': 'bedrock-vectordb-rag-383',\n",
      "                            'standbyReplicas': 'ENABLED',\n",
      "                            'status': 'CREATING',\n",
      "                            'type': 'VECTORSEARCH'}}\n"
     ]
    }
   ],
   "source": [
    "pp(collection)\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'94qa58u59f50h2eiuwp3.us-east-1.aoss.amazonaws.com'\n"
     ]
    }
   ],
   "source": [
    "collection_id = collection['createCollectionDetail']['id']\n",
    "host = collection_id + '.' + region_name + '.aoss.amazonaws.com'\n",
    "pp(host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opensearch serverless arn:  arn:aws:iam::507922848584:policy/AmazonBedrockOSSPolicyForKnowledgeBase_600\n"
     ]
    }
   ],
   "source": [
    "# create oss policy and attach it to Bedrock execution role\n",
    "create_oss_policy_attach_bedrock_execution_role(collection_id=collection_id,\n",
    "                                                bedrock_kb_execution_role=bedrock_kb_execution_role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Step 2 - Create vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = auth = AWSV4SignerAuth(credentials, region_name, service)\n",
    "\n",
    "index_name = f\"bedrock-sample-index-{suffix}\"\n",
    "body_json = {\n",
    "   \"settings\": {\n",
    "      \"index.knn\": \"true\"\n",
    "   },\n",
    "   \"mappings\": {\n",
    "      \"properties\": {\n",
    "         \"vector\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 1536,\n",
    "            \"method\": {\n",
    "                \"name\": \"hnsw\",\n",
    "                \"engine\": \"faiss\",  \n",
    "                \"space_type\": \"l2\",\n",
    "                \"parameters\": {\n",
    "                    \"ef_construction\": 200,\n",
    "                    \"m\": 16\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "         \"text\": {\n",
    "            \"type\": \"text\"\n",
    "         },\n",
    "         \"text-metadata\": {\n",
    "            \"type\": \"text\"         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "# Build the OpenSearch client\n",
    "oss_client = OpenSearch(\n",
    "    hosts=[{'host': host, 'port': 443}],\n",
    "    http_auth=awsauth,\n",
    "    use_ssl=True,\n",
    "    verify_certs=True,\n",
    "    connection_class=RequestsHttpConnection,\n",
    "    timeout=300\n",
    ")\n",
    "# # It can take up to a minute for data access rules to be enforced\n",
    "time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating index:\n",
      "{'acknowledged': True,\n",
      " 'index': 'bedrock-sample-index-383',\n",
      " 'shards_acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "# Create index\n",
    "response = oss_client.indices.create(index=index_name, body=json.dumps(body_json))\n",
    "print('\\nCreating index:')\n",
    "pp(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Create Knowledge Base\n",
    "Steps:\n",
    "- initialize Open search serverless configuration which will include collection ARN, index name, vector field, text field and metadata field.\n",
    "- initialize chunking strategy, based on which KB will split the documents into pieces of size equal to the chunk size mentioned in the `chunkingStrategyConfiguration`.\n",
    "- initialize the web URL configuration, which will be used to create the data source object later.\n",
    "- initialize the Titan embeddings model ARN, as this will be used to create the embeddings for each of the text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearchServerlessConfiguration = {\n",
    "            \"collectionArn\": collection[\"createCollectionDetail\"]['arn'],\n",
    "            \"vectorIndexName\": index_name,\n",
    "            \"fieldMapping\": {\n",
    "                \"vectorField\": \"vector\",\n",
    "                \"textField\": \"text\",\n",
    "                \"metadataField\": \"text-metadata\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "# # Differet Chunking Strategies  \n",
    "# # FIXED_SIZE Chunking\n",
    "# chunkingStrategyConfiguration = {\n",
    "#                                     \"chunkingStrategy\": \"FIXED_SIZE\",\n",
    "#                                     \"fixedSizeChunkingConfiguration\": {\n",
    "#                                                                             \"maxTokens\": 512,\n",
    "#                                                                             \"overlapPercentage\": 20\n",
    "#                                                                         }\n",
    "#                                 }\n",
    "\n",
    "# HIERARCHICAL Chunking \n",
    "chunkingStrategyConfiguration = {\n",
    "                                    \"chunkingStrategy\": \"HIERARCHICAL\",      \n",
    "                                    \"hierarchicalChunkingConfiguration\": {  \n",
    "                                                                            'levelConfigurations': [\n",
    "                                                                                {\n",
    "                                                                                    'maxTokens': 1500\n",
    "                                                                                },\n",
    "                                                                                {\n",
    "                                                                                    'maxTokens': 300\n",
    "                                                                                }\n",
    "                                                                            ],\n",
    "                                                                            'overlapTokens': 60\n",
    "                                                                        }\n",
    "                                }\n",
    "\n",
    "# # SEMANTIC Chunking \n",
    "# chunkingStrategyConfiguration = {\n",
    "#                                     \"semanticChunkingConfiguration\": {          \n",
    "#                                                                          'breakpointPercentileThreshold': 95,\n",
    "#                                                                          'bufferSize': 1,\n",
    "#                                                                          'maxTokens': 300\n",
    "#                                                                      }\n",
    "#                                 }\n",
    "\n",
    "\n",
    "## Differet Data Source \n",
    "# Web URL \n",
    "# my_url = <ENTER YOUR WEB URL> \n",
    "# webConfiguration = {\"sourceConfiguration\": {\n",
    "#                           \"urlConfiguration\": {\n",
    "#                            \"seedUrls\": [{\n",
    "#                                     \"url\": my_url                  \n",
    "#                                 }]\n",
    "#                             }\n",
    "#                         },\n",
    "#                      \"crawlerConfiguration\": {\n",
    "#                             \"crawlerLimits\": {\n",
    "#                                 \"rateLimit\": 50\n",
    "#                             },\n",
    "#                             \"scope\": \"HOST_ONLY\"\n",
    "#                         }\n",
    "#                    }         \n",
    "\n",
    "# S3 \n",
    "s3Configuration = {\n",
    "    \"bucketArn\": f\"arn:aws:s3:::{bucket_name}\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingModelArn = f\"arn:aws:bedrock:{region_name}::foundation-model/amazon.titan-embed-text-v1\"\n",
    "\n",
    "name = f\"bedrock-sample-knowledge-base-{suffix}\"\n",
    "description = \"Bedrock Knowledge Bases for Web URL and S3 Connector\"\n",
    "roleArn = bedrock_kb_execution_role_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Provide the above configurations as input to the `create_knowledge_base` method, which will create the Knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KnowledgeBase\n",
    "from retrying import retry\n",
    "\n",
    "@retry(wait_random_min=1000, wait_random_max=2000,stop_max_attempt_number=7)\n",
    "def create_knowledge_base_func():\n",
    "    create_kb_response = bedrock_agent_client.create_knowledge_base(\n",
    "                                                                        name = name,\n",
    "                                                                        description = description,\n",
    "                                                                        roleArn = roleArn,\n",
    "                                                                        knowledgeBaseConfiguration = {\n",
    "                                                                                                        \"type\": \"VECTOR\",\n",
    "                                                                                                        \"vectorKnowledgeBaseConfiguration\": {\n",
    "                                                                                                            \"embeddingModelArn\": embeddingModelArn\n",
    "                                                                                                        }\n",
    "                                                                                                    },\n",
    "                                                                        storageConfiguration = {\n",
    "                                                                                                    \"type\": \"OPENSEARCH_SERVERLESS\",\n",
    "                                                                                                    \"opensearchServerlessConfiguration\":opensearchServerlessConfiguration\n",
    "                                                                                                }\n",
    "                                                                    )\n",
    "    return create_kb_response[\"knowledgeBase\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    kb = create_knowledge_base_func()\n",
    "except Exception as err:\n",
    "    print(f\"{err=}, {type(err)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createdAt': datetime.datetime(2024, 7, 18, 19, 50, 9, 937932, tzinfo=tzutc()),\n",
      " 'description': 'Bedrock Knowledge Bases for Web URL and S3 Connector',\n",
      " 'knowledgeBaseArn': 'arn:aws:bedrock:us-east-1:507922848584:knowledge-base/DARMEMCMOS',\n",
      " 'knowledgeBaseConfiguration': {'type': 'VECTOR',\n",
      "                                'vectorKnowledgeBaseConfiguration': {'embeddingModelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1'}},\n",
      " 'knowledgeBaseId': 'DARMEMCMOS',\n",
      " 'name': 'bedrock-sample-knowledge-base-383',\n",
      " 'roleArn': 'arn:aws:iam::507922848584:role/AmazonBedrockExecutionRoleForKnowledgeBase_600',\n",
      " 'status': 'CREATING',\n",
      " 'storageConfiguration': {'opensearchServerlessConfiguration': {'collectionArn': 'arn:aws:aoss:us-east-1:507922848584:collection/94qa58u59f50h2eiuwp3',\n",
      "                                                                'fieldMapping': {'metadataField': 'text-metadata',\n",
      "                                                                                 'textField': 'text',\n",
      "                                                                                 'vectorField': 'vector'},\n",
      "                                                                'vectorIndexName': 'bedrock-sample-index-383'},\n",
      "                          'type': 'OPENSEARCH_SERVERLESS'},\n",
      " 'updatedAt': datetime.datetime(2024, 7, 18, 19, 50, 9, 937932, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "pp(kb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get KnowledgeBase \n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId = kb['knowledgeBaseId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Next we need to create a data source, which will be associated with the knowledge base created above. Once the data source is ready, we can then start to ingest the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Just for documentation :) \n",
    "# help(bedrock_agent_client.create_data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'createdAt': datetime.datetime(2024, 7, 18, 19, 50, 18, 982893, tzinfo=tzutc()),\n",
      " 'dataDeletionPolicy': 'DELETE',\n",
      " 'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::my-kb-dataset-test-bucket-2024'},\n",
      "                             'type': 'S3'},\n",
      " 'dataSourceId': 'Y6SFR9NSQB',\n",
      " 'description': 'Bedrock Knowledge Bases for Web URL and S3 Connector',\n",
      " 'knowledgeBaseId': 'DARMEMCMOS',\n",
      " 'name': 'bedrock-sample-knowledge-base-383',\n",
      " 'status': 'AVAILABLE',\n",
      " 'updatedAt': datetime.datetime(2024, 7, 18, 19, 50, 18, 982893, tzinfo=tzutc()),\n",
      " 'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'HIERARCHICAL',\n",
      "                                                            'hierarchicalChunkingConfiguration': {'levelConfigurations': [{'maxTokens': 1500},\n",
      "                                                                                                                          {'maxTokens': 300}],\n",
      "                                                                                                  'overlapTokens': 60}}}}\n"
     ]
    }
   ],
   "source": [
    "# Create a S3 DataSource in KnowledgeBase \n",
    "create_ds_response = bedrock_agent_client.create_data_source(\n",
    "                                                                name = name,\n",
    "                                                                description = description,\n",
    "                                                                knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "                                                                dataDeletionPolicy = 'DELETE',\n",
    "                                                                dataSourceConfiguration = {\n",
    "                                                                    # # For S3 \n",
    "                                                                    \"type\": \"S3\",\n",
    "                                                                    \"s3Configuration\" : s3Configuration\n",
    "                                                                    # # For Web URL \n",
    "                                                                    # \"type\": \"WEB\",\n",
    "                                                                    # \"webConfiguration\":webConfiguration                                                                    \n",
    "                                                                },\n",
    "                                                                vectorIngestionConfiguration = {\n",
    "                                                                    \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "                                                                }\n",
    "                                                            )\n",
    "ds = create_ds_response[\"dataSource\"]\n",
    "pp(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '35b665b2-b12a-4e68-b5f3-4424cbf2f83b',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'date': 'Thu, 18 Jul 2024 19:50:19 GMT',\n",
       "   'content-type': 'application/json',\n",
       "   'content-length': '657',\n",
       "   'connection': 'keep-alive',\n",
       "   'x-amzn-requestid': '35b665b2-b12a-4e68-b5f3-4424cbf2f83b',\n",
       "   'x-amz-apigw-id': 'bH6_RGJcoAMEuow=',\n",
       "   'x-amzn-trace-id': 'Root=1-669971fb-64809b30540a52cf4963a86e'},\n",
       "  'RetryAttempts': 0},\n",
       " 'dataSource': {'createdAt': datetime.datetime(2024, 7, 18, 19, 50, 18, 982893, tzinfo=tzutc()),\n",
       "  'dataDeletionPolicy': 'DELETE',\n",
       "  'dataSourceConfiguration': {'s3Configuration': {'bucketArn': 'arn:aws:s3:::my-kb-dataset-test-bucket-2024'},\n",
       "   'type': 'S3'},\n",
       "  'dataSourceId': 'Y6SFR9NSQB',\n",
       "  'description': 'Bedrock Knowledge Bases for Web URL and S3 Connector',\n",
       "  'knowledgeBaseId': 'DARMEMCMOS',\n",
       "  'name': 'bedrock-sample-knowledge-base-383',\n",
       "  'status': 'AVAILABLE',\n",
       "  'updatedAt': datetime.datetime(2024, 7, 18, 19, 50, 18, 982893, tzinfo=tzutc()),\n",
       "  'vectorIngestionConfiguration': {'chunkingConfiguration': {'chunkingStrategy': 'HIERARCHICAL',\n",
       "    'hierarchicalChunkingConfiguration': {'levelConfigurations': [{'maxTokens': 1500},\n",
       "      {'maxTokens': 300}],\n",
       "     'overlapTokens': 60}}}}}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get S3 DataSource \n",
    "bedrock_agent_client.get_data_source(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Start ingestion job\n",
    "Once the KB and data source is created, we can start the ingestion job.\n",
    "During the ingestion job, KB will fetch the documents in the data source, pre-process it to extract text, chunk it based on the chunking size provided, create embeddings of each chunk and then write it to the vector database, in this case OSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start an ingestion job\n",
    "start_job_response = bedrock_agent_client.start_ingestion_job(knowledgeBaseId = kb['knowledgeBaseId'], dataSourceId = ds[\"dataSourceId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataSourceId': 'Y6SFR9NSQB',\n",
      " 'ingestionJobId': 'K83WPI3TED',\n",
      " 'knowledgeBaseId': 'DARMEMCMOS',\n",
      " 'startedAt': datetime.datetime(2024, 7, 18, 19, 50, 29, 695810, tzinfo=tzutc()),\n",
      " 'statistics': {'numberOfDocumentsDeleted': 0,\n",
      "                'numberOfDocumentsFailed': 0,\n",
      "                'numberOfDocumentsScanned': 0,\n",
      "                'numberOfMetadataDocumentsModified': 0,\n",
      "                'numberOfMetadataDocumentsScanned': 0,\n",
      "                'numberOfModifiedDocumentsIndexed': 0,\n",
      "                'numberOfNewDocumentsIndexed': 0},\n",
      " 'status': 'STARTING',\n",
      " 'updatedAt': datetime.datetime(2024, 7, 18, 19, 50, 29, 695810, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "job = start_job_response[\"ingestionJob\"]\n",
    "pp(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataSourceId': 'Y6SFR9NSQB',\n",
      " 'ingestionJobId': 'K83WPI3TED',\n",
      " 'knowledgeBaseId': 'DARMEMCMOS',\n",
      " 'startedAt': datetime.datetime(2024, 7, 18, 19, 50, 29, 695810, tzinfo=tzutc()),\n",
      " 'statistics': {'numberOfDocumentsDeleted': 0,\n",
      "                'numberOfDocumentsFailed': 0,\n",
      "                'numberOfDocumentsScanned': 4,\n",
      "                'numberOfMetadataDocumentsModified': 0,\n",
      "                'numberOfMetadataDocumentsScanned': 0,\n",
      "                'numberOfModifiedDocumentsIndexed': 0,\n",
      "                'numberOfNewDocumentsIndexed': 4},\n",
      " 'status': 'COMPLETE',\n",
      " 'updatedAt': datetime.datetime(2024, 7, 18, 19, 50, 48, 228320, tzinfo=tzutc())}\n"
     ]
    }
   ],
   "source": [
    "# Get job \n",
    "while(job['status']!='COMPLETE' ):\n",
    "  get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "      knowledgeBaseId = kb['knowledgeBaseId'],\n",
    "        dataSourceId = ds[\"dataSourceId\"],\n",
    "        ingestionJobId = job[\"ingestionJobId\"]\n",
    "  )\n",
    "  job = get_job_response[\"ingestionJob\"]\n",
    "pp(job)\n",
    "time.sleep(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DARMEMCMOS\n"
     ]
    }
   ],
   "source": [
    "kb_id = kb[\"knowledgeBaseId\"]\n",
    "print(kb_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'kb_id' (str)\n"
     ]
    }
   ],
   "source": [
    "%store kb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Test the knowledge base\n",
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# try out KB using RetrieveAndGenerate API\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"                                           # <Change it to any model of your choice which is supported by KB>\n",
    "model_arn = f'arn:aws:bedrock:us-east-1::foundation-model/{model_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon has been investing heavily in Large Language Models (LLMs) and Generative AI, which it believes will transform and improve virtually every customer experience. Amazon has been working on its own LLMs for a while and plans to continue investing substantially in these models across all of its consumer, seller, brand, and creator experiences. Additionally, Amazon is democratizing this technology through AWS so that companies of all sizes can leverage Generative AI by offering price-performant machine learning chips and enabling companies to choose from various LLMs and build applications with AWS features. Regarding Amazon's performance in the last couple of years, the letter mentions that despite 2022 being a challenging macroeconomic year, Amazon still found a way to grow demand and innovate in its largest businesses. However, it also faced some operating challenges and had to make adjustments in investment decisions. Overall, the CEO expresses optimism that Amazon will emerge stronger from this period, citing its large market opportunities in areas like retail and cloud computing where it still has relatively low market share compared to the total addressable market.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Amazon doing towards generative AI ? and how is Amazon's performance in last couple of years ?\"\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        'text': query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        'type': 'KNOWLEDGE_BASE',\n",
    "        'knowledgeBaseConfiguration': {\n",
    "            'knowledgeBaseId': kb_id,\n",
    "            'modelArn': model_arn\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "generated_text = response['output']['text']\n",
    "\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Grocery is a big growth opportunity for Amazon.   Amazon Business is another example of an investment where our ecommerce and logistics capabilities position us well to pursue this large market segment. Amazon Business allows businesses, municipalities, and organizations to procure products like office supplies and other bulk items easily and at great savings. While some areas of the economy have struggled over the past few years, Amazon Business has thrived. Why? Because the team has translated what it means to deliver selection, value, and convenience into a business procurement setting, constantly listening to and learning from customers, and innovating on their behalf. Some people have never heard of Amazon Business, but, our business customers love it. Amazon Business launched in 2015 and today drives roughly $35B in annualized gross sales. More than six million active customers, including 96 of the global Fortune 100 companies, are enjoying Amazon Business’ one-stop shopping, real-time analytics, and broad selection on hundreds of millions of business supplies. We believe that we’ve only scratched the surface of what’s possible to date, and plan to keep building the features our business customers tell us they need and want.   While many brands and merchants successfully sell their products on Amazon’s marketplace, there are also a large number of brands and sellers who have launched their own direct-to-consumer websites. One of the challenges for these merchants is driving conversion from views to purchases. We invented Buy with Prime to help with this challenge. Buy with Prime allows third-party brands and sellers to offer their products on their own websites to our large Amazon Prime membership, and offer those customers fast, free Prime shipping and seamless checkout with their Amazon account. Buy with Prime provides merchants several additional benefits, including Amazon handling the product storage, picking, packing, delivery, payment, and any returns, all through Amazon Pay and Fulfillment by Amazon. Buy with Prime has recently been made available to all US merchants; and so far, Buy with Prime has increased shopper conversion on third-party shopping sites by 25% on average. Merchants are excited about converting more sales and fulfilling these shipments more easily, Prime members love that they can use their Prime benefits on more destinations, and Buy with Prime allows us to improve the shopping experience across more of the web.   Expanding internationally, pursuing large retail market segments that are still nascent for Amazon, and using our unique assets to help merchants sell more effectively on their own websites are somewhat natural extensions for us. There are also a few investments we’re making that are further from our core businesses, but where we see unique opportunity. In 2003, AWS would have been a classic example. In 2023, Amazon Healthcare and Kuiper are potential analogues.   Our initial efforts in Healthcare began with pharmacy, which felt less like a major departure from ecommerce. For years, Amazon customers had asked us when we’d offer them an online pharmacy as their frustrations mounted with current providers. Launched in 2020, Amazon Pharmacy is a full-service, online pharmacy that offers transparent pricing, easy refills, and savings for Prime members. The business is growing quickly, and continues to innovate. An example is Amazon Pharmacy’s recent launch of RxPass, which for a $5 per        month flat fee, enables Prime members to get as many of the eligible prescription medications as they need for dozens of common conditions, like high blood pressure, acid reflux, and anxiety. However, our customers have continued to express a strong desire for Amazon to provide a better alternative to the inefficient and unsatisfying broader healthcare experience. We decided to start with primary care as it’s a prevalent first stop in the patient journey. We evaluated and studied the existing landscape extensively, including some early Amazon experiments like Amazon Care. During this process, we identified One Medical’s patient-focused experience as an excellent foundation upon which to build our future business; and in July 2022, we announced our acquisition of One Medical. There are several elements that customers love about One Medical. It has a fantastic digital app that makes it easy for patients to discuss issues with a medical practitioner via chat or video conference. If a physical visit is required, One Medical has offices in cities across the US where patients can book same or next day appointments. One Medical has relationships with specialty physicians in each of its cities and works closely with local hospital systems to make seeing specialists easy, so One Medical members can quickly access these resources when needed. Going forward, we strongly believe that One Medical and Amazon will continue to innovate together to change what primary care will look like for customers.   Kuiper is another example of Amazon innovating for customers over the long term in an area where there’s high customer need. Our vision for Kuiper is to create a low-Earth orbit satellite system to deliver high-quality broadband internet service to places around the world that don’t currently have it. There are hundreds of millions of households and businesses who don’t have reliable access to the internet. Imagine what they’ll be able to do with reliable connectivity, from people taking online education courses, using financial services, starting their own businesses, doing their shopping, enjoying entertainment, to businesses and governments improving their coverage, efficiency, and operations. Kuiper will deliver not only accessibility, but affordability. Our teams have developed low-cost antennas (i.e. customer terminals) that will lower the barriers to access. We recently unveiled the new terminals that will communicate with the satellites passing overhead, and we expect to be able to produce our standard residential version for less than $400 each. They’re small: 11 inches square, 1 inch thick, and weigh less than 5 pounds without their mounting bracket, but they deliver speeds up to 400 megabits per second. And they’re powered by Amazon-designed baseband chips. We’re preparing to launch two prototype satellites to test the entire end-to-end communications network this year, and plan to be in beta with commercial customers in 2024. The customer reaction to what we’ve shared thus far about Kuiper has been very positive, and we believe Kuiper represents a very large potential opportunity for Amazon. It also shares several similarities to AWS in that it’s capital intensive at the start, but has a large prospective consumer, enterprise, and government customer base, significant revenue and operating profit potential, and relatively few companies with the technical and inventive aptitude, as well as the investment hypothesis to go after it.   One final investment area that I’ll mention, that’s core to setting Amazon up to invent in every area of our business for many decades to come, and where we’re investing heavily is Large Language Models (“LLMs”) and Generative AI. Machine learning has been a technology with high promise for several decades, but it’s only been the last five to ten years that it’s started to be used more pervasively by companies. This shift was driven by several factors, including access to higher volumes of compute capacity at lower prices than was ever available. Amazon has been using machine learning extensively for 25 years, employing it in everything from personalized ecommerce recommendations, to fulfillment center pick paths, to drones for Prime Air, to Alexa, to the many machine learning services AWS offers (where AWS has the broadest machine learning functionality and customer base of any cloud provider). ', 'Dear shareholders:   As I sit down to write my second annual shareholder letter as CEO, I find myself optimistic and energized by what lies ahead for Amazon. Despite 2022 being one of the harder macroeconomic years in recent memory, and with some of our own operating challenges to boot, we still found a way to grow demand (on top of the unprecedented growth we experienced in the first half of the pandemic). We innovated in our largest businesses to meaningfully improve customer experience short and long term. And, we made important adjustments in our investment decisions and the way in which we’ll invent moving forward, while still preserving the long-term investments that we believe can change the future of Amazon for customers, shareholders, and employees.   While there were an unusual number of simultaneous challenges this past year, the reality is that if you operate in large, dynamic, global market segments with many capable and well-funded competitors (the conditions in which Amazon operates all of its businesses), conditions rarely stay stagnant for long.   In the 25 years I’ve been at Amazon, there has been constant change, much of which we’ve initiated ourselves. When I joined Amazon in 1997, we had booked $15M in revenue in 1996, were a books-only retailer, did not have a third-party marketplace, and only shipped to addresses in the US. Today, Amazon sells nearly every physical and digital retail item you can imagine, with a vibrant third-party seller ecosystem that accounts for 60% of our unit sales, and reaches customers in virtually every country around the world. Similarly, building a business around a set of technology infrastructure services in the cloud was not obvious in 2003 when we started pursuing AWS, and still wasn’t when we launched our first services in 2006. Having virtually every book at your fingertips in 60 seconds, and then being able to store and retrieve them on a lightweight digital reader was not “a thing” yet when we launched Kindle in 2007, nor was a voice-driven personal assistant like Alexa (launched in 2014) that you could use to access entertainment, control your smart home, shop, and retrieve all sorts of information.   There have also been times when macroeconomic conditions or operating inefficiencies have presented us with new challenges. For instance, in the 2001 dot-com crash, we had to secure letters of credit to buy inventory for the holidays, streamline costs to deliver better profitability for the business, yet still prioritized the long-term customer experience and business we were trying to build (if you remember, we actually lowered prices in most of our categories during that tenuous 2001 period). You saw this sort of balancing again in 2008-2009 as we endured the recession provoked by the mortgage-backed securities financial crisis. We took several actions to manage the cost structure and efficiency of our Stores business, but we also balanced this streamlining with investment in customer experiences that we believed could be substantial future businesses with strong returns for shareholders. In 2008, AWS was still a fairly small, fledgling business. We knew we were on to something, but it still required substantial capital investment. There were voices inside and outside of the company questioning why Amazon (known mostly as an online retailer then) would be investing so much in cloud computing. But, we knew we were inventing something special that could create a lot of value for customers and Amazon in the future. We had a head start on potential competitors; and if anything, we wanted to accelerate our pace of innovation. We made the long-term decision to continue investing in AWS. Fifteen years later, AWS is now an $85B annual revenue run rate business, with strong profitability, that has transformed how customers from start-ups to multinational companies to public sector organizations manage their technology infrastructure. Amazon would be a different company if we’d slowed investment in AWS during that 2008-2009 period.   Change is always around the corner. Sometimes, you proactively invite it in, and sometimes it just comes a-knocking. But, when you see it’s coming, you have to embrace it. And, the companies that do this well over a long period of time usually succeed. I’m optimistic about our future prospects because I like the way our team is responding to the changes we see in front of us.        Over the last several months, we took a deep look across the company, business by business, invention by invention, and asked ourselves whether we had conviction about each initiative’s long-term potential to drive enough revenue, operating income, free cash flow, and return on invested capital. In some cases, it led to us shuttering certain businesses. For instance, we stopped pursuing physical store concepts like our Bookstores and 4 Star stores, closed our Amazon Fabric and Amazon Care efforts, and moved on from some newer devices where we didn’t see a path to meaningful returns. In other cases, we looked at some programs that weren’t producing the returns we’d hoped (e.g. free shipping for all online grocery orders over $35) and amended them. We also reprioritized where to spend our resources, which ultimately led to the hard decision to eliminate 27,000 corporate roles. There are a number of other changes that we’ve made over the last several months to streamline our overall costs, and like most leadership teams, we’ll continue to evaluate what we’re seeing in our business and proceed adaptively.   We also looked hard at how we were working together as a team and asked our corporate employees to come back to the office at least three days a week, beginning in May. During the pandemic, our employees rallied to get work done from home and did everything possible to keep up with the unexpected circumstances that presented themselves. It was impressive and I’m proud of the way our collective team came together to overcome unprecedented challenges for our customers, communities, and business. But, we don’t think it’s the best long-term approach. We’ve become convinced that collaborating and inventing is easier and more effective when we’re working together and learning from one another in person. The energy and riffing on one another’s ideas happen more freely, and many of the best Amazon inventions have had their breakthrough moments from people staying behind after a meeting and working through ideas on a whiteboard, or continuing the conversation on the walk back from a meeting, or just popping by a teammate’s office later that day with another thought. Invention is often messy. It wanders and meanders and marinates. Serendipitous interactions help it, and there are more of those in-person than virtually. It’s also significantly easier to learn, model, practice, and strengthen our culture when we’re in the office together most of the time and surrounded by our colleagues. Innovation and our unique culture have been incredibly important in our first 29 years as a company, and I expect it will be comparably so in the next 29.   A critical challenge we’ve continued to tackle is the rising cost to serve in our Stores fulfillment network (i.e. the cost to get a product from Amazon to a customer)—and we’ve made several changes that we believe will meaningfully improve our fulfillment costs and speed of delivery.   ']\n"
     ]
    }
   ],
   "source": [
    "## print out the source attribution/citations from the original documents to see if the response generated belongs to the context.\n",
    "citations = response[\"citations\"]\n",
    "contexts = []\n",
    "for citation in citations:\n",
    "    retrievedReferences = citation[\"retrievedReferences\"]\n",
    "    for reference in retrievedReferences:\n",
    "        contexts.append(reference[\"content\"][\"text\"])\n",
    "\n",
    "print(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Retrieve API\n",
    "Retrieve API converts user queries into embeddings, searches the knowledge base, and returns the relevant results, giving you more control to build custom workﬂows on top of the semantic search results. The output of the Retrieve API includes the the retrieved text chunks, the location type and URI of the source data, as well as the relevance scores of the retrievals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# retreive api for fetching only the relevant context.\n",
    "query = \"How many new positions were opened across Amazon's fulfillment and delivery network?\" \n",
    "\n",
    "relevant_documents = bedrock_agent_runtime_client.retrieve(\n",
    "    retrievalQuery= {\n",
    "        'text': query\n",
    "    },\n",
    "    knowledgeBaseId=kb_id,\n",
    "    retrievalConfiguration= {\n",
    "        'vectorSearchConfiguration': {\n",
    "            'numberOfResults': 3 # will fetch top 3 documents which matches closely with the query.\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': {'text': 'Dear shareholders:   Over the past 25 years at Amazon, I’ve had the opportunity to write many narratives, emails, letters, and keynotes for employees, customers, and partners. But, this is the first time I’ve had the honor of writing our annual shareholder letter as CEO of Amazon. Jeff set the bar high on these letters, and I will try to keep them worth reading.   When the pandemic started in early 2020, few people thought it would be as expansive or long-running as it’s been. Whatever role Amazon played in the world up to that point became further magnified as most physical venues shut down for long periods of time and people spent their days at home. This meant that hundreds of millions of people relied on Amazon for PPE, food, clothing, and various other items that helped them navigate this unprecedented time. Businesses and governments also had to shift, practically overnight, from working with colleagues and technology on-premises to working remotely. AWS played a major role in enabling this business continuity. Whether companies saw extraordinary demand spikes, or demand diminish quickly with reduced external consumption, the cloud’s elasticity to scale capacity up and down quickly, as well as AWS’s unusually broad functionality helped millions of companies adjust to these difficult circumstances.   Our AWS and Consumer businesses have had different demand trajectories during the pandemic. In the first year of the pandemic, AWS revenue continued to grow at a rapid clip—30% year over year (“YoY”) in 2020 on a $35 billion annual revenue base in 2019—but slower than the 37% YoY growth in 2019. This was due in part to the uncertainty and slowing demand that so many businesses encountered, but also in part to our helping companies optimize their AWS footprint to save money. Concurrently, companies were stepping back and determining what they wanted to change coming out of the pandemic. Many concluded that they didn’t want to continue managing their technology infrastructure themselves, and made the decision to accelerate their move to the cloud. This shift by so many companies (along with the economy recovering) helped re-accelerate AWS’s revenue growth to 37% YoY in 2021.   Conversely, our Consumer revenue grew dramatically in 2020. In 2020, Amazon’s North America and International Consumer revenue grew 39% YoY on the very large 2019 revenue base of $245 billion; and, this extraordinary growth extended into 2021 with revenue increasing 43% YoY in Q1 2021. These are astounding numbers. We realized the equivalent of three years’ forecasted growth in about 15 months.   As the world opened up again starting in late Q2 2021, and more people ventured out to eat, shop, and travel, consumer spending returned to being spread over many more entities. We weren’t sure what to expect in 2021, but the fact that we continued to grow at double digit rates (with a two-year Consumer compounded annual growth rate of 29%) was encouraging as customers appreciated the role Amazon played for them during the pandemic, and started using Amazon for a larger amount of their household purchases.   This growth also created short-term logistics and cost challenges. We spent Amazon’s first 25 years building a very large fulfillment network, and then had to double it in the last 24 months to meet customer demand. As we were bringing this new capacity online, the labor market tightened considerably, making it challenging both to receive all of the inventory our vendors and sellers wanted to send us and to place that inventory as close to customers as we typically do. Combined with ocean, air, and trucking capacity becoming scarcer and more expensive, this created extra transportation and productivity costs. Supply chains were disrupted in ways none of us had seen previously. We hoped that the major impact from COVID-19 would recede as 2021 drew to a close, but then omicron reared its head in December, which had worldwide ramifications, including impacting people’s ability to work. And then in late February, with Russia’s invasion of Ukraine, fuel costs and inflation became bigger issues with which to contend.   So, 2021 was a crazy and unpredictable year, continuing a trend from 2020. But, I’m proud of the incredible commitment and effort from our employees all over the world. I’m not sure any of us would have gotten        through the pandemic the same way without the dedication and extraordinary efforts shown by our teams during this period, and I’m eternally grateful.   It’s not normal for a company of any size to be able to respond to something as discontinuous and unpredictable as this pandemic turned out to be. What is it about Amazon that made it possible for us to do so? It’s because we weren’t starting from a standing start. We had been iterating on and remaking our fulfillment capabilities for nearly two decades. In every business we pursue, we’re constantly experimenting and inventing. We’re divinely discontented with customer experiences, whether they’re our own or not. We believe these customer experiences can always be better, and we strive to make customers’ lives better and easier every day. The beauty of this mission is that you never run out of runway; customers always want better, and our job is both to listen to their feedback and to imagine what else is possible and invent on their behalf.   People often assume that the game-changing inventions they admire just pop out of somebody’s head, a light bulb goes off, a team executes to that idea, and presto—you have a new invention that’s a breakaway success for a long time. That’s rarely, if ever, how it happens. One of the lesser known facts about innovative companies like Amazon is that they are relentlessly debating, re-defining, tinkering, iterating, and experimenting to take the seed of a big idea and make it into something that resonates with customers and meaningfully changes their customer experience over a long period of time.   Let me give you some Amazon examples.   Our Fulfillment Network: Going back to the pandemic, there’s no way we could have started working on our fulfillment network in March 2020 and satisfied anything close to what our customers needed. We’d been innovating in our fulfillment network for 20 years, constantly trying to shorten the time to get items to customers. In the early 2000s, it took us an average of 18 hours to get an item through our fulfillment centers and on the right truck for shipment. Now, it takes us two. To deliver as reliably and cost-effectively as we desire, and to serve Amazon Prime members expecting shipments in a couple of days, we spent years building out an expansive set of fulfillment centers, a substantial logistics and transportation capability, and reconfigured how we did virtually everything in our facilities. For perspective, in 2004, we had seven fulfillment centers in the U.S. and four in other parts of the world, and we hadn’t yet added delivery stations, which connect our fulfillment and sortation centers to the last-mile delivery vans you see driving around your neighborhood. Fast forward to the end of 2021, we had 253 fulfillment centers, 110 sortation centers, and 467 delivery stations in North America, with an additional 157 fulfillment centers, 58 sortation centers, and 588 delivery stations across the globe. Our delivery network grew to more than 260,000 drivers worldwide, and our Amazon Air cargo fleet has more than 100 aircraft. '}, 'location': {'s3Location': {'uri': 's3://my-kb-dataset-test-bucket-2024/2021-Shareholder-Letter.pdf'}, 'type': 'S3'}, 'metadata': {'x-amz-bedrock-kb-source-uri': 's3://my-kb-dataset-test-bucket-2024/2021-Shareholder-Letter.pdf', 'x-amz-bedrock-kb-data-source-id': 'Y6SFR9NSQB'}, 'score': 0.62179804}, {'content': {'text': 'Fast forward to the end of 2021, we had 253 fulfillment centers, 110 sortation centers, and 467 delivery stations in North America, with an additional 157 fulfillment centers, 58 sortation centers, and 588 delivery stations across the globe. Our delivery network grew to more than 260,000 drivers worldwide, and our Amazon Air cargo fleet has more than 100 aircraft. This has represented a capital investment of over $100 billion and countless iterations and small process improvements by over a million Amazonians in the last decade and a half.   Ironically, just before COVID started, we’d made the decision to invest billions of incremental dollars over several years to deliver an increasing number of Prime shipments in one day. This initiative was slowed by the challenges of the pandemic, but we’ve since resumed our focus here. Delivering a substantial amount of shipments in one day is hard (especially across the millions of items that we offer) and initially expensive as we build out the infrastructure to scale this efficiently. But, we believe our over 200 million Prime customers, who will tell you very clearly that faster is almost always better, will love this. So, this capability to ship millions of items within a couple days (and increasingly one day) was not from one aha moment and not developed in a year or two. It’s been hard-earned by putting ourselves in the shoes of our customers, knowing what they wanted, organizing Amazonians to work together to invent better solutions, and investing a large amount of financial and people resources over 20 years (often well in advance of when it would pay out). This type of iterative innovation is never finished and has periodic peaks in investment years, but leads to better long-term customer experiences, customer loyalty, and returns for our shareholders.   AWS: As we were defining AWS and working backwards on the services we thought customers wanted, we kept triggering one of the biggest tensions in product development—where to draw the line on functionality in V1. One early meeting in particular—for our core compute service called Elastic Compute Cloud (“EC2”)— was scheduled for an hour, and took three, as we animatedly debated whether we could launch a compute service without an accompanying persistent block storage companion (a form of network attached storage).        Everybody agreed that having a persistent block store was important to a complete compute service; however, to have one ready would take an extra year. The question became could we offer customers a useful service where they could get meaningful value before we had all the features we thought they wanted? We decided that the initial launch of EC2 could be feature-poor if we also organized ourselves to listen to customers and iterate quickly. This approach works well if you indeed iterate quickly; but, is disastrous if you can’t. We launched EC2 in 2006 with one instance size, in one data center, in one region of the world, with Linux operating system instances only (no Windows), without monitoring, load balancing, auto-scaling, or yes, persistent storage. EC2 was an initial success, but nowhere near the multi-billion-dollar service it’s become until we added the missing capabilities listed above, and then some.   In the early days of AWS, people sometimes asked us why compute wouldn’t just be an undifferentiated commodity. But, there’s a lot more to compute than just a server. Customers want various flavors of compute (e.g. server configurations optimized for storage, memory, high-performance compute, graphics rendering, machine learning), multiple form factors (e.g. fixed instance sizes, portable containers, serverless functions), various sizes and optimizations of persistent storage, and a slew of networking capabilities. Then, there’s the CPU chip that runs in your compute. For many years, the industry had used Intel or AMD x86 processors. We have important partnerships with these companies, but realized that if we wanted to push price and performance further (as customers requested), we’d have to develop our own chips, too. Our first generalized chip was Graviton, which we announced in 2018. This helped a subset of customer workloads run more cost-effectively than prior options. But, it wasn’t until 2020, after taking the learnings from Graviton and innovating on a new chip, that we had something remarkable with our Graviton2 chip, which provides up to 40% better price-performance than the comparable latest generation x86 processors. Think about how much of an impact 40% improvement on compute is. Compute is used for every bit of technology. That’s a huge deal for customers. And, while Graviton2 has been a significant success thus far (48 of the top 50 AWS EC2 customers have already adopted it), the AWS Chips team was already learning from what customers said could be better, and announced Graviton3 this past December (offering a 25% improvement on top of Graviton2’s relative gains). The list of what we’ve invented and delivered for customers in EC2 (and AWS in general) is pretty mind-boggling, and this iterative approach to innovation has not only given customers much more functionality in AWS than they can find anywhere else (which is a significant differentiator), but also allowed us to arrive at the much more game-changing offering that AWS is today.   Devices: Our first foray into devices was the Kindle, released in 2007. It was not the most sophisticated industrial design (it was creamy white in color and the corners were uncomfortable for some people to hold), but revolutionary because it offered customers the ability to download any of over 90,000 books (now millions) in 60 seconds—and we got better and faster at building attractive designs. Shortly thereafter, we launched a tablet, and then a phone (with the distinguishing feature of having front-facing cameras and a gyroscope to give customers a dynamic perspective along with varied 3D experiences). The phone was unsuccessful, and though we determined we were probably too late to this party and directed these resources elsewhere, we hired some fantastic long-term builders and learned valuable lessons from this failure that have served us well in devices like Echo and FireTV.   When I think of the first Echo device—and what Alexa could do for customers at that point—it was noteworthy, yet so much less capable than what’s possible today. Today, there are hundreds of millions of Alexa-enabled devices out there (in homes, offices, cars, hotel rooms, Amazon Echo devices, and third-party manufacturer devices); you can listen to music—or watch videos now; you can control your lights and home automation; you can create routines like “Start My Day” where Alexa tells you the weather, your estimated commute time based on current traffic, then plays the news; you can easily order retail items on Amazon; you can get general or customized news, updates on sporting events and related stats—and we’re still quite early with respect to what Alexa and Alexa-related devices will do for customers. Our goal is for Alexa to be the world’s most helpful and resourceful personal assistant, who makes people’s lives meaningfully easier and better. We have a lot more inventing and iterating to go, but customers continue to indicate that we’re on the right path. We have several other devices at varying stages of evolution (e.g. '}, 'location': {'s3Location': {'uri': 's3://my-kb-dataset-test-bucket-2024/2021-Shareholder-Letter.pdf'}, 'type': 'S3'}, 'metadata': {'x-amz-bedrock-kb-source-uri': 's3://my-kb-dataset-test-bucket-2024/2021-Shareholder-Letter.pdf', 'x-amz-bedrock-kb-data-source-id': 'Y6SFR9NSQB'}, 'score': 0.611476}, {'content': {'text': 'A critical challenge we’ve continued to tackle is the rising cost to serve in our Stores fulfillment network (i.e. the cost to get a product from Amazon to a customer)—and we’ve made several changes that we believe will meaningfully improve our fulfillment costs and speed of delivery.   During the early part of the pandemic, with many physical stores shut down, our consumer business grew at an extraordinary clip, with annual revenue increasing from $245B in 2019 to $434B in 2022. This meant that we had to double the fulfillment center footprint that we’d built over the prior 25 years and substantially accelerate building a last-mile transportation network that’s now the size of UPS (along with a new sortation center network to assist with efficiency and speed when items needed to traverse long distances)—all in the span of about two years. This was no easy feat, and hundreds of thousands of Amazonians worked very hard to make this happen. However, not surprisingly, with that rate and scale of change, there was a lot of optimization needed to yield the intended productivity. Over the last several months, we’ve scrutinized every process path in our fulfillment centers and transportation network and redesigned scores of processes and mechanisms, resulting in steady productivity gains and cost reductions over the last few quarters. There’s more work to do, but we’re pleased with our trajectory and the meaningful upside in front of us.   We also took this occasion to make larger structural changes that set us up better to deliver lower costs and faster speed for many years to come. A good example was reevaluating how our US fulfillment network was organized. Until recently, Amazon operated one national US fulfillment network that distributed inventory from fulfillment centers spread across the entire country. If a local fulfillment center didn’t have the product a customer ordered, we’d end up shipping it from other parts of the country, costing us more and increasing delivery times. This challenge became more pronounced as our fulfillment network expanded to hundreds of additional nodes over the last few years, distributing inventory across more locations and increasing the complexity of connecting the fulfillment center and delivery station nodes efficiently. Last year, we started rearchitecting our inventory placement strategy and leveraging our larger fulfillment center footprint to move from a national fulfillment network to a regionalized network model. We made significant internal changes (e.g. placement and logistics software, processes, physical operations) to create eight interconnected regions in smaller geographic areas. Each of these regions has broad, relevant selection to operate in a largely self- sufficient way, while still being able to ship nationally when necessary. Some of the most meaningful and hard        work came from optimizing the connections between this large amount of infrastructure. We also continue to improve our advanced machine learning algorithms to better predict what customers in various parts of the country will need so that we have the right inventory in the right regions at the right time. We’ve recently completed this regional roll out and like the early results. Shorter travel distances mean lower cost to serve, less impact on the environment, and customers getting their orders faster. On the latter, we’re excited about seeing more next day and same-day deliveries, and we’re on track to have our fastest Prime delivery speeds ever in 2023. Overall, we remain confident about our plans to lower costs, reduce delivery times, and build a meaningfully larger retail business with healthy operating margins.   AWS has an $85B annualized revenue run rate, is still early in its adoption curve, but at a juncture where it’s critical to stay focused on what matters most to customers over the long-haul. Despite growing 29% year-over- year (“YoY”) in 2022 on a $62B revenue base, AWS faces short-term headwinds right now as companies are being more cautious in spending given the challenging, current macroeconomic conditions. While some companies might obsess over how they could extract as much money from customers as possible in these tight times, it’s neither what customers want nor best for customers in the long term, so we’re taking a different tack. One of the many advantages of AWS and cloud computing is that when your business grows, you can seamlessly scale up; and conversely, if your business contracts, you can choose to give us back that capacity and cease paying for it. This elasticity is unique to the cloud, and doesn’t exist when you’ve already made expensive capital investments in your own on-premises datacenters, servers, and networking gear. In AWS, like all our businesses, we’re not trying to optimize for any one quarter or year. We’re trying to build customer relationships (and a business) that outlast all of us; and as a result, our AWS sales and support teams are spending much of their time helping customers optimize their AWS spend so they can better weather this uncertain economy. Many of these AWS customers tell us that they’re not cost-cutting as much as cost- optimizing so they can take their resources and apply them to emerging and inventive new customer experiences they’re planning. Customers have appreciated this customer-focused, long-term approach, and we think it’ll bode well for both customers and AWS.   While these short-term headwinds soften our growth rate, we like a lot of the fundamentals that we’re seeing in AWS. Our new customer pipeline is robust, as are our active migrations. Many companies use discontinuous periods like this to step back and determine what they strategically want to change, and we find an increasing number of enterprises opting out of managing their own infrastructure, and preferring to move to AWS to enjoy the agility, innovation, cost-efficiency, and security benefits. And most importantly for customers, AWS continues to deliver new capabilities rapidly (over 3,300 new features and services launched in 2022), and invest in long-term inventions that change what’s possible.   Chip development is a good example. In last year’s letter, I mentioned the investment we were making in our general-purpose CPU processors named Graviton. Graviton2-based compute instances deliver up to 40% better price-performance than the comparable latest generation x86-based instances; and in 2022, we delivered our Graviton3 chips, providing 25% better performance than the Graviton2 processors. Further, as machine learning adoption has continued to accelerate, customers have yearned for lower-cost GPUs (the chips most commonly used for machine learning). AWS started investing years ago in these specialized chips for machine learning training and inference (inferences are the predictions or answers that a machine learning model provides). We delivered our first training chip in 2022 (“Trainium”); and for the most common machine learning models, Trainium-based instances are up to 140% faster than GPU-based instances at up to 70% lower cost. Most companies are still in the training stage, but as they develop models that graduate to large-scale production, they’ll find that most of the cost is in inference because models are trained periodically whereas inferences are happening all the time as their associated application is being exercised. We launched our first inference chips (“Inferentia”) in 2019, and they have saved companies like Amazon over a hundred million dollars in capital expense already. Our Inferentia2 chip, which just launched, offers up to four times higher throughput and ten times lower latency than our first Inferentia processor. '}, 'location': {'s3Location': {'uri': 's3://my-kb-dataset-test-bucket-2024/2022-Shareholder-Letter.pdf'}, 'type': 'S3'}, 'metadata': {'x-amz-bedrock-kb-source-uri': 's3://my-kb-dataset-test-bucket-2024/2022-Shareholder-Letter.pdf', 'x-amz-bedrock-kb-data-source-id': 'Y6SFR9NSQB'}, 'score': 0.58126783}]\n"
     ]
    }
   ],
   "source": [
    "print(relevant_documents[\"retrievalResults\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Clean up\n",
    "Please make sure to comment the below section if you are planning to use the Knowledge Base that you created above for building your RAG application.\n",
    "If you only wanted to try out creating the KB using SDK, then please make sure to delete all the resources that were created as you will be incurred cost for storing documents in OSS index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Delete KnowledgeBase\n",
    "# bedrock_agent_client.delete_data_source(dataSourceId = ds[\"dataSourceId\"], knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "# bedrock_agent_client.delete_knowledge_base(knowledgeBaseId=kb['knowledgeBaseId'])\n",
    "# oss_client.indices.delete(index=index_name)\n",
    "# aoss_client.delete_collection(id=collection_id)\n",
    "# aoss_client.delete_access_policy(type=\"data\", name=access_policy['accessPolicyDetail']['name'])\n",
    "# aoss_client.delete_security_policy(type=\"network\", name=network_policy['securityPolicyDetail']['name'])\n",
    "# aoss_client.delete_security_policy(type=\"encryption\", name=encryption_policy['securityPolicyDetail']['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete role and policies\n",
    "# from utility import delete_iam_role_and_policies\n",
    "# delete_iam_role_and_policies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
